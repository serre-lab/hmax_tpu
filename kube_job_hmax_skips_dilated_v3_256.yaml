apiVersion: batch/v1
kind: Job
metadata:
  name: hmax-dilated-tpu-v3-256
spec:
  # parallelism: 8  # Matches number of preemptables. This is the queue size.
  template:
    metadata:
      annotations:
        # The Cloud TPUs that will be created for this Job will support
        # TensorFlow 2.2. This version MUST match the
        # TensorFlow version that your model is built on.
        tf-version.cloud-tpus.google.com: "2.1"
    spec:
      securityContext:
        runAsUser: 0
      restartPolicy: OnFailure
      containers:
      - name: resnet-tpu
        # The official TensorFlow 1.15.2 image.
        # https://hub.docker.com/r/tensorflow/tensorflow
        # image: tensorflow/tensorflow:1.15.2
        image: gcr.io/kubeflow-images-public/tensorflow-2.1.0-notebook-gpu:1.0.0
        command: ["/bin/sh"]
        args:
          - -c
          - >-
            git clone https://github.com/drewlinsley/hmax_tpu.git &&
            cd hmax_tpu &&
            pip3 install --upgrade pip &&
            pip3 install -U tpunicorn &&
            pip3 install -r requirements.txt &&
            export PATH="$HOME/.local/bin:$PATH" &&
            pu list &&
            EXP_NAME=hmax_skips_dilated_tpuv3_256 &&
            MODEL_NAME=hmax_model_skips_dilated &&
            bash jobs/dilated_hmax_supervised_tpuv3_256.sh $EXP_NAME $(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS) $MODEL_NAME
        resources:
          limits:
            # Request a single Preemptible v2-8 Cloud TPU device to train the
            # model. A single v2-8 Cloud TPU device consists of 4 chips, each of
            # which has 2 cores, so there are 8 cores in total.
            cloud-tpus.google.com/preemptible-v3: 256
